# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10FTORQim5ZrF2dMIfuKi1OWY9GjipF0d

# Homework: Fine-Tuning GPT for Story Generation

You have previously learned to use the Hugging Face library with a BERT pre-trained model. For this assignment, you will use a pre-trained GPT model and fine-tune it for a story generation task. The dataset you will use is available here: https://www.kaggle.com/datasets/annbengardt/fairy-tales-from-around-the-world/data

###Download dataset
"""
import subprocess
import os
import random
import shutil

# Step 1: Install the kaggle package
subprocess.check_call(["pip", "install", "kaggle"])

# Step 2: Download the dataset using the kaggle API
os.system("kaggle datasets download -d annbengardt/fairy-tales-from-around-the-world -p ~/nlp-hw5")

# Step 3: Unzip the downloaded dataset
import zipfile

# Path to the downloaded zip file
zip_path = os.path.expanduser("~/nlp-hw5/fairy-tales-from-around-the-world.zip")
extract_to = os.path.expanduser("~/nlp-hw5/fairy_tales")

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

# Step 4: Define source and destination directories
source_dir = os.path.join(extract_to, "fairy_tales")  # Adjust based on the actual folder structure after extraction
train_dir = os.path.join(extract_to, "train")
validation_dir = os.path.join(extract_to, "validation")

# Step 5: Create train and validation directories if they don't exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)

# Step 6: Get a list of all .txt files
all_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]

# Step 7: Shuffle and split files for 80/20 train-validation
random.shuffle(all_files)
split_index = int(0.8 * len(all_files))
train_files = all_files[:split_index]
validation_files = all_files[split_index:]

# Step 8: Move files to train and validation directories
for file_name in train_files:
    shutil.move(os.path.join(source_dir, file_name), os.path.join(train_dir, file_name))
for file_name in validation_files:
    shutil.move(os.path.join(source_dir, file_name), os.path.join(validation_dir, file_name))

print(f"Moved {len(train_files)} files to the train directory and {len(validation_files)} files to the validation directory.")



import os

def convert_to_utf8(directory):
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        if filename.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)

# Convert train and validation folders to UTF-8 encoding
convert_to_utf8(os.path.expanduser("~/nlp-hw5/fairy_tales/train"))
convert_to_utf8(os.path.expanduser("~/nlp-hw5/fairy_tales/validation"))


#!pip install datasets

# Replace !pip install datasets with:
import subprocess
subprocess.check_call(["pip", "install", "--upgrade", "datasets"])
subprocess.check_call(["pip", "install", "datasets"])


from datasets import load_dataset

# Load all .txt files from the train and validation directories
datasets = load_dataset("text", data_files={
    "train": os.path.expanduser("~/nlp-hw5/fairy_tales/train/*.txt"),
    "validation": os.path.expanduser("~/nlp-hw5/fairy_tales/validation/*.txt")
})


"""###Task 1: Define your tokenization and text processing steps."""

from transformers import GPT2Tokenizer

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set the pad_token to eos_token
tokenizer.pad_token = tokenizer.eos_token

# Define a function for tokenizing text
def tokenize_function(examples):
    tokenized_inputs=tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

# Tokenize train and validation datasets
tokenized_datasets = datasets.map(tokenize_function, batched=True)

"""###Task 2 Describe your training strategy and how you will calculate the loss during training.

The training strategy is using Hugging Face’s `Trainer` class to streamline the fine-tuning of a pre-trained GPT-2 model for the story generation task. The main objective is to train the model to predict the next token in a sequence accurately, allowing it to generate coherent and contextually appropriate stories based on a prompt.

1. **Data Preparation**:
   - **Tokenization**: Use the `GPT2Tokenizer` to convert text data into tokens, making it compatible with the model. Tokenization involves truncating or padding sequences to a specified length, ensuring that input sequences have uniform length. This facilitates efficient batch processing during training.
   - **Training and Validation Split**: Divide the data into training (80%) and validation (20%) sets to monitor performance. The training set guides the model’s learning, while the validation set helps evaluate the model’s performance at the end of each epoch.

2. **Training Configuration**:
   - **Batch Size**: We set a batch size of 4 per device to balance between memory efficiency and training speed.
   - **Epochs**: The model will run for 3 epochs, which should provide sufficient exposure to the dataset while minimizing the risk of overfitting.
   - **Evaluation Strategy**: The model is evaluated at the end of each epoch, allowing us to monitor improvements and prevent overfitting by tracking validation loss.

3. **Checkpointing and Logging**:
   - We save model checkpoints every 10,000 steps, with a limit of two recent checkpoints to manage storage.
   - Logs are stored to track the training process, including metrics like loss, which help in diagnosing issues or adjusting parameters if needed.

### Loss Calculation

For this story generation task, **cross-entropy loss** is the loss function used to evaluate the model’s performance:

- **Cross-Entropy Loss**: This loss measures the accuracy of token predictions by comparing the predicted probability distribution over possible tokens with the actual token in the sequence.
  - The model generates a probability distribution over possible next tokens at each step. Cross-entropy loss penalizes the model more heavily for large deviations from the correct token.
  - Minimizing this loss effectively trains the model to predict tokens in the sequence more accurately, leading to coherent story generation.

Using the `Trainer` class, the cross-entropy loss calculation is automatically handled during training. After each batch, the Trainer computes the loss, updates the model weights to minimize it, and saves these adjustments. At the end of each epoch, the validation loss is computed to assess how well the model generalizes to unseen data.

###Task 3: Define your GPT-based model. Describe any additional fine-tuning layers you have added.

`TrainingArguments` allows to control various aspects of the training process, including the batch size, number of epochs, and checkpointing.
"""

from transformers import TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",              # Directory to save model checkpoints
    evaluation_strategy="epoch",         # Evaluate the model at the end of each epoch
    per_device_train_batch_size=4,       # Batch size for training (per device)
    per_device_eval_batch_size=4,        # Batch size for evaluation (per device)
    num_train_epochs=3,                  # Number of times to go over the entire training dataset
    save_steps=10_000,                   # Save a checkpoint every 10,000 steps
    save_total_limit=2,                  # Keep only the last 2 checkpoints, discarding older ones
    logging_dir='./logs'                 # Directory for saving logs
)

"""Using `GPT2LMHeadModel` from Hugging Face’s library allows you to build on a pre-trained language model (GPT-2) for tasks like story generation. Loading a pre-trained model leverages previous training on large datasets, which improves performance and reduces training time.

"""

from transformers import GPT2LMHeadModel

# Load the pre-trained model
model = GPT2LMHeadModel.from_pretrained("gpt2")

"""The `Trainer` class combines the model, dataset, and training arguments into one cohesive training loop. It also handles calculating the loss at each step automatically. The loss function measures how well or poorly the model is performing, with the goal being to minimize this loss over time."""

from transformers import Trainer

# Initialize Trainer
trainer = Trainer(
    model=model,                         # Model to train
    args=training_args,                  # Training arguments defined above
    train_dataset=tokenized_datasets["train"],  # Tokenized training dataset
    eval_dataset=tokenized_datasets["validation"] # Tokenized validation dataset
)

import torch.nn as nn

class CustomGPT2Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.gpt2 = GPT2LMHeadModel.from_pretrained("gpt2")
        self.additional_layer = nn.Linear(self.gpt2.config.hidden_size, self.gpt2.config.hidden_size)

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels)
        logits = self.additional_layer(outputs.logits)
        return (logits, ) if labels is None else (logits, outputs.loss)

model = CustomGPT2Model()

"""###Task4"""

import os
os.environ["WANDB_DISABLED"] = "true"

# Train the model
train_result = trainer.train()

# Save the trained model
trainer.save_model("./fine_tuned_model")

# Get the training loss and plot the loss curve
import matplotlib.pyplot as plt

if "loss" in train_result.metrics:
    train_loss = train_result.metrics["loss"]
    steps = list(range(len(train_loss)))

    plt.plot(steps, train_loss)
    plt.xlabel("Training Steps")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.show()
else:
    print("Loss data unavailble")

"""###Task 5: Use your trained model to generate a short story using the following prompt:
prompt = "Once upon a time in a faraway land, there was a young prince who"
"""

# Define the prompt
prompt = "Once upon a time in a faraway land, there was a young prince who"
inputs = tokenizer(prompt, return_tensors="pt")

# Generate story
output = model.generate(inputs.input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)

# Decode and print the generated story
generated_story = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_story)
